{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Demo: Multi-Turn RL with ASAN Rewards\n",
        "\n",
        "This notebook demonstrates how to use ASAN predictions as reward signals for multi-turn RL training to optimize for long-term conversational safety.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "\n",
        "# Change to parent directory to run as package\n",
        "notebook_dir = os.path.dirname(os.path.abspath(''))\n",
        "parent_dir = os.path.dirname(notebook_dir)\n",
        "os.chdir(parent_dir)\n",
        "sys.path.insert(0, parent_dir)\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import Dict, List\n",
        "\n",
        "# Import ASAN components\n",
        "from models.asan_predictor import ASANPredictor, ASANConfig\n",
        "from rl_training.asan_reward_model import ASANRewardModel, RewardConfig\n",
        "from rl_training.long_horizon_rewards import LongHorizonRewardComputer, LongHorizonConfig\n",
        "from rl_training.multi_turn_environment import MultiTurnSafetyEnvironment, EnvironmentConfig, UserType\n",
        "from rl_training.trajectory_replay_buffer import TrajectoryReplayBuffer, ReplayBufferConfig\n",
        "\n",
        "print(\"Imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Initialize Components\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize ASAN predictor\n",
        "asan_config = ASANConfig()\n",
        "asan_predictor = ASANPredictor(asan_config)\n",
        "asan_predictor.eval()\n",
        "\n",
        "# Initialize ASAN reward model\n",
        "reward_config = RewardConfig(\n",
        "    safety_weight=1.0,\n",
        "    trajectory_smoothness_weight=0.2,\n",
        "    early_detection_bonus_weight=0.3\n",
        ")\n",
        "asan_reward_model = ASANRewardModel(asan_predictor, reward_config)\n",
        "\n",
        "# Initialize long-horizon reward computer\n",
        "horizon_config = LongHorizonConfig(horizon_length=5)\n",
        "horizon_computer = LongHorizonRewardComputer(asan_predictor, horizon_config)\n",
        "\n",
        "print(\"Components initialized\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Create Multi-Turn Conversation Trajectories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multi_turn_trajectory(num_turns=5, gradually_harmful=False, num_layers=12):\n",
        "    \"\"\"Create a multi-turn conversation trajectory\n",
        "    \n",
        "    Args:\n",
        "        num_turns: Number of conversation turns\n",
        "        gradually_harmful: Whether to gradually increase harm across turns\n",
        "        num_layers: Number of transformer layers (default 12 to match ASANConfig)\n",
        "    \"\"\"\n",
        "    conversation = []\n",
        "    \n",
        "    for turn in range(num_turns):\n",
        "        # Create trajectory for this turn\n",
        "        trajectory = {\n",
        "            'attention_patterns': {},\n",
        "            'hidden_states': {},\n",
        "            'token_probs': []\n",
        "        }\n",
        "        \n",
        "        # Simulate 10 timesteps per turn\n",
        "        num_timesteps = 10\n",
        "        \n",
        "        # Create attention patterns (12 layers to match ASANConfig expectation)\n",
        "        for layer_idx in range(num_layers):\n",
        "            layer_attentions = []\n",
        "            for t in range(num_timesteps):\n",
        "                seq_len = 10\n",
        "                if gradually_harmful:\n",
        "                    # Gradually increase harm probability\n",
        "                    harm_level = (turn + 1) / num_turns\n",
        "                    if harm_level > 0.7:\n",
        "                        attn = torch.zeros(seq_len, seq_len)\n",
        "                        attn[0, :] = 1.0 / seq_len\n",
        "                    else:\n",
        "                        attn = torch.ones(seq_len, seq_len) / seq_len\n",
        "                else:\n",
        "                    attn = torch.ones(seq_len, seq_len) / seq_len\n",
        "                layer_attentions.append(attn)\n",
        "            trajectory['attention_patterns'][layer_idx] = layer_attentions\n",
        "        \n",
        "        # Create hidden states (12 layers to match ASANConfig expectation)\n",
        "        hidden_dim = 256\n",
        "        for layer_idx in range(num_layers):\n",
        "            layer_states = []\n",
        "            for t in range(num_timesteps):\n",
        "                seq_len = 10\n",
        "                if gradually_harmful:\n",
        "                    harm_level = (turn + 1) / num_turns\n",
        "                    hidden = torch.randn(seq_len, hidden_dim) * (0.5 + harm_level * 1.5)\n",
        "                else:\n",
        "                    hidden = torch.randn(seq_len, hidden_dim) * 0.5\n",
        "                layer_states.append(hidden)\n",
        "            trajectory['hidden_states'][layer_idx] = layer_states\n",
        "        \n",
        "        # Create token probabilities\n",
        "        vocab_size = 50257\n",
        "        for t in range(num_timesteps):\n",
        "            probs = torch.softmax(torch.randn(vocab_size), dim=0)\n",
        "            trajectory['token_probs'].append(probs)\n",
        "        \n",
        "        conversation.append(trajectory)\n",
        "    \n",
        "    return conversation\n",
        "\n",
        "# Create test conversations\n",
        "safe_conversation = create_multi_turn_trajectory(num_turns=5, gradually_harmful=False)\n",
        "harmful_conversation = create_multi_turn_trajectory(num_turns=5, gradually_harmful=True)\n",
        "\n",
        "print(f\"Created conversations with {len(safe_conversation)} turns each\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Compute Trajectory Rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute rewards for single turn\n",
        "safe_turn = safe_conversation[0]\n",
        "safe_reward = asan_reward_model.compute_trajectory_reward(safe_turn)\n",
        "\n",
        "harmful_turn = harmful_conversation[-1]\n",
        "harmful_reward = asan_reward_model.compute_trajectory_reward(harmful_turn)\n",
        "\n",
        "print(\"Single Turn Rewards:\")\n",
        "print(f\"Safe turn - Total reward: {safe_reward['total_reward']:.4f}\")\n",
        "print(f\"  Safety: {safe_reward['component_rewards']['safety']:.4f}\")\n",
        "print(f\"  Smoothness: {safe_reward['component_rewards']['smoothness']:.4f}\")\n",
        "print(f\"  Early detection: {safe_reward['component_rewards']['early_detection']:.4f}\")\n",
        "\n",
        "print(f\"\\nHarmful turn - Total reward: {harmful_reward['total_reward']:.4f}\")\n",
        "print(f\"  Safety: {harmful_reward['component_rewards']['safety']:.4f}\")\n",
        "print(f\"  Smoothness: {harmful_reward['component_rewards']['smoothness']:.4f}\")\n",
        "print(f\"  Early detection: {harmful_reward['component_rewards']['early_detection']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Compute Long-Horizon Conversation Rewards\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute conversation-level rewards\n",
        "safe_conv_reward = horizon_computer.compute_conversation_reward(safe_conversation)\n",
        "harmful_conv_reward = horizon_computer.compute_conversation_reward(harmful_conversation)\n",
        "\n",
        "print(\"Conversation-Level Rewards:\")\n",
        "print(f\"\\nSafe conversation:\")\n",
        "print(f\"  Conversation reward: {safe_conv_reward['conversation_reward']:.4f}\")\n",
        "print(f\"  Turn-level rewards: {[f'{r:.3f}' for r in safe_conv_reward['turn_level_rewards']]}\")\n",
        "\n",
        "print(f\"\\nHarmful conversation:\")\n",
        "print(f\"  Conversation reward: {harmful_conv_reward['conversation_reward']:.4f}\")\n",
        "print(f\"  Turn-level rewards: {[f'{r:.3f}' for r in harmful_conv_reward['turn_level_rewards']]}\")\n",
        "\n",
        "# Detect multi-turn exploitation\n",
        "exploitation = horizon_computer.detect_multi_turn_exploitation(harmful_conversation)\n",
        "print(f\"\\nMulti-turn exploitation detection:\")\n",
        "print(f\"  Trust building: {exploitation.get('trust_building', False)}\")\n",
        "print(f\"  Gradual context shift: {exploitation.get('gradual_context_shift', False)}\")\n",
        "print(f\"  Exploitation attempt: {exploitation.get('exploitation_attempt', False)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Temporal Credit Assignment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# When harm occurs at turn 4, assign credit to earlier turns\n",
        "harm_occurred_turn = 4\n",
        "credits = horizon_computer.temporal_credit_assignment(harmful_conversation, harm_occurred_turn)\n",
        "\n",
        "print(f\"Temporal Credit Assignment (harm at turn {harm_occurred_turn}):\")\n",
        "for turn_idx, credit in sorted(credits.items()):\n",
        "    credit_type = \"Contributed to harm\" if credit < 0 else \"Contributed to safety\"\n",
        "    print(f\"  Turn {turn_idx}: {credit:.4f} ({credit_type})\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test Multi-Turn Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize environment\n",
        "env_config = EnvironmentConfig(\n",
        "    max_turns=5,\n",
        "    normal_user_prob=0.7,\n",
        "    mild_adversarial_prob=0.2,\n",
        "    highly_adversarial_prob=0.1\n",
        ")\n",
        "\n",
        "env = MultiTurnSafetyEnvironment(asan_predictor, asan_reward_model, env_config)\n",
        "\n",
        "# Run a few episodes\n",
        "for episode in range(3):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    turn = 0\n",
        "    \n",
        "    print(f\"\\nEpisode {episode + 1}:\")\n",
        "    print(f\"  User type: {state['user_type'].value}\")\n",
        "    \n",
        "    while not done and turn < 3:\n",
        "        # Simulate model response (placeholder)\n",
        "        action = {\n",
        "            'state': state,\n",
        "            'response': {'text': f'Model response at turn {turn}'},\n",
        "            'trajectory': safe_conversation[min(turn, len(safe_conversation)-1)] if state['user_type'] == UserType.NORMAL else harmful_conversation[min(turn, len(harmful_conversation)-1)]\n",
        "        }\n",
        "        \n",
        "        state, reward, done, info = env.step(action)\n",
        "        total_reward += reward\n",
        "        turn += 1\n",
        "        \n",
        "        print(f\"  Turn {turn}: reward={reward:.4f}, harm_prob={info.get('harm_probability', 0.0):.4f}\")\n",
        "    \n",
        "    print(f\"  Total episode reward: {total_reward:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Test Replay Buffer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize replay buffer\n",
        "replay_config = ReplayBufferConfig(\n",
        "    buffer_size=100,\n",
        "    priority_sampling=True\n",
        ")\n",
        "replay_buffer = TrajectoryReplayBuffer(replay_config)\n",
        "\n",
        "# Add some trajectories\n",
        "for i in range(10):\n",
        "    trajectory = safe_conversation[i % len(safe_conversation)] if i % 2 == 0 else harmful_conversation[i % len(harmful_conversation)]\n",
        "    reward = asan_reward_model.compute_trajectory_reward(trajectory)['total_reward']\n",
        "    harm_prob = 0.2 if i % 2 == 0 else 0.8\n",
        "    replay_buffer.add(trajectory, reward, harm_prob)\n",
        "\n",
        "# Get statistics\n",
        "stats = replay_buffer.get_statistics()\n",
        "print(\"Replay Buffer Statistics:\")\n",
        "print(f\"  Size: {stats['size']}\")\n",
        "print(f\"  Harmful ratio: {stats['harmful_ratio']:.2f}\")\n",
        "print(f\"  Safe ratio: {stats['safe_ratio']:.2f}\")\n",
        "print(f\"  Avg reward: {stats['avg_reward']:.4f}\")\n",
        "print(f\"  Avg harm prob: {stats['avg_harm_prob']:.4f}\")\n",
        "\n",
        "# Sample a batch\n",
        "if replay_buffer.is_ready():\n",
        "    batch = replay_buffer.sample(5)\n",
        "    print(f\"\\nSampled batch of {len(batch)} trajectories\")\n",
        "    print(f\"  First trajectory reward: {batch[0]['reward']:.4f}\")\n",
        "\n",
        "print(\"\\nMulti-turn RL demo completed successfully!\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
